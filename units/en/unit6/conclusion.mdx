# Conclusion [[conclusion]]

Congrats on finishing this unit and the tutorial. You've just trained your first virtual robots ðŸ¥³.

**Take time to grasp the material before continuing**. You can also look at the additional reading materials we provided in the *additional reading* section.

Feel free to train your agent in other environments. The **best way to learn is to try things on your own!** For instance, what about teaching your robot [to stack objects](https://panda-gym.readthedocs.io/en/latest/usage/environments.html#sparce-reward-end-effector-control-default-setting)?

In the next unit, we will learn to improve Actor-Critic Methods with Proximal Policy Optimization using the [CleanRL library](https://github.com/vwxyzjn/cleanrl). Then we'll study how to speed up the process with the [Sample Factory library](https://samplefactory.dev/). You'll train your PPO agents in these environments: VizDoom, Racing Car, and a 3D FPS.

TODO: IMAGE of the environment Vizdoom + ED

Finally, with your feedback, we want **to improve and update the course iteratively**. If you have some, please ðŸ‘‰  [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)

### Keep learning, stay awesome ðŸ¤—,
