# Hands-on

<CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
notebooks={[
  {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit7/unit7.ipynb"}
  ]}
  askForHelpUrl="http://hf.co/join/discord" />


Now that you learned the bases of multi-agents. You're ready to train our first agents in a multi-agents system: **a 2vs2 soccer team that needs to beat the opponent team**.

And youâ€™re going to participate in AI vs. AI challenges where your trained agent will compete against other classmatesâ€™ agents every day and be ranked on a new leaderboard.

To validate this hands-on for the certification process, you just need to push your trained model. There are no minimal result to attain to validate it.

For more information about the certification process, check this section ðŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process


This hands-on will be different, since to get correct results you need to train your agents for 4h to 5h. And given the risk of timeout in colab we advise you to train on your own computer.

Let's get started,


## What is AI vs. AI ?

AI vs. AI is a tool we developed at Hugging Face.
It's a matchmaking algorithm  where your pushed models are ranked by playing against other models.

AI vs. AI is three tools:

- A *matchmaking process* defining which model against which model and running the model fights using a background task in the Space.
- A *leaderboard* getting the match history results and displaying the models ELO ratings: https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos
- A *Space demo* to visualize your agents playing against others : https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos


We're going to write a blogpost to explain this AI vs. AI tool in detail, but to give you the big picture it works this way:
- Every 4h, our algorithm fetch all the available models for a given environment.
- It creates a queue of matches with the matchmaking algorithm.
- Simulate the match in a Unity headless process and gather the match result (1 if first model won, 0.5 if itâ€™s a draw, 0 if the second model won) in a Dataset.
- Then, when all matches from the matches queue are done, we update the elo score for each model and update the leaderboard.

### Competition Rules

This first AI vs. AI competition is an experiment, the goal is to improve the tool in the future with your feedback. So some **breakups can happen during the challenge**. But don't worry
**all the results are saved in a dataset so we can always restart the calculation correctly without loosing information**.

In order that your model get correctly evaluated against others you need to follow these rules:

1. You can't change the observation space or action space. By doing that your model will not work in our evaluation.
2. You can't use a custom trainer for now, you need to use Unity MLAgents.
3. We provide executables to train your agents, you can also use the Unity Editor if you prefer **but in order to avoid bugs we advise you to use our executables**.

What will make the difference during this challenge are **the hyperparameters you choose**.

# Step 0: Install MLAgents and download the correct executable

You need to install a specific version of MLAgents




## Step 1: Understand the environment

The environment is called `` it was made by the Unity MLAgents Team.

The goal in this

The goal in this environment is to train our agent to **get the gold brick on the top of the Pyramid. To do that, it needs to press a button to spawn a Pyramid, navigate to the Pyramid, knock it over, and move to the gold brick at the top**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.png" alt="Pyramids Environment"/>


## The reward function

The reward function is:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-reward.png" alt="Pyramids Environment"/>

In terms of code, it looks like this
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-reward-code.png" alt="Pyramids Reward"/>

To train this new agent that seeks that button and then the Pyramid to destroy, weâ€™ll use a combination of two types of rewards:

- The *extrinsic one* given by the environment (illustration above).
- But also an *intrinsic* one called **curiosity**. This second will **push our agent to be curious, or in other terms, to better explore its environment**.

If you want to know more about curiosity, the next section (optional) will explain the basics.

## The observation space

In terms of observation, we **use 148 raycasts that can each detect objects** (switch, bricks, golden brick, and walls.)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids_raycasts.png"/>

We also use a **boolean variable indicating the switch state** (did we turn on or off the switch to spawn the Pyramid) and a vector that **contains the agentâ€™s speed**.

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-obs-code.png" alt="Pyramids obs code"/>


## The action space

The action space is **discrete** with four possible actions:

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-action.png" alt="Pyramids Environment"/>









## MA POCA
https://arxiv.org/pdf/2111.05992.pdf




- EXE
